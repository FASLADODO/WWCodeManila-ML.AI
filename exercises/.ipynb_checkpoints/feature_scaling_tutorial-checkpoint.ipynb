{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature scaling\n",
    "\n",
    "Different features can be measured on different scales. For example:\n",
    " * height can be measure in centimeters\n",
    " * weight in kilograms\n",
    " * blood pressure in mmHg\n",
    " * etc. \n",
    "\n",
    "Some classifiers combine and compare feature values, e.g. computing distance using Euclidean distance. The problem is that if one of the features has a broad range of values, the distance will be governed by this particular feature! For example, consider the two features:\n",
    "* the percentage of unemployment in a city - ranges from 0.0 to 1.0\n",
    "* the population of the city - can range up to 500,000\n",
    "\n",
    "In this example, the percentage will be swamped by the population. \n",
    "\n",
    "<b>Feature scaling</b> transforms the data so that the features have, more or less, uniform range. <i>The main advantage of scaling is to avoid attributes in greater numeric ranges dominating those in smaller numeric ranges.</i>\n",
    "\n",
    "As we'll see later, scaling features can lead to a faster optimization process and better results. However, for other algorithms like the Decision Tree Classifier, scaling is *not* necessary (i.e. it's scale-invariant). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wine Dataset\n",
    "About the dataset: `wine.csv`\n",
    "\n",
    "These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines\n",
    "\n",
    "The data consists of:\n",
    "- 13 features (see dataset/wine.headers)\n",
    "- 3 classes - types of wine (I'm not sure what these types particularly are. In the dataset, they're just denoted as 1, 2, and 3.) \n",
    "\n",
    "See more detail here: https://archive.ics.uci.edu/ml/datasets/wine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get started\n",
    "Let's start by importing the scikit-learn package for data preprocessing `sklearn.preprocessing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>1.)</b> Import all the libraries that we've used in previous sessions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be plotting some things so let's go ahead and import `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2.)</b> Load the dataset `wine.csv` located in the /datasets folder. Set `header=None` since the dataset has no headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = None\n",
    "df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The headers are actually stored in the file /datasets/wine.names. We load this file as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "headers = []\n",
    "with open('../datasets/wine.names','r') as file:\n",
    "    for line in file:\n",
    "        headers.append(line.strip()) # appends each line to the 'headers' list. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `strip()` strips each line of line breaks (\\n) and trailing white spaces. We then set the headers as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Uncomment this to set the headers\n",
    "# df.columns = headers #Sets the headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>3.)</b> Explore and inspect your data. Use the methods and techniques you've learned in the section `Exploratory Analysis`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>4.)</b> Create matrix `X` containing the features and target vector `y` containing the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = None\n",
    "y = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 8.)</b> Split the dataset into training and testing data.\n",
    "* Set the train size to 80%, and hold back 20% for testing \n",
    "* For random_state, set the seed to 42 (para uniform results natin lahat :D)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = None, None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization \n",
    "Some machine learning algorithms behave badly when their individual features don't look like standard normally distributed data (i.e. Gaussian distribution with zero mean and unit standard deviation). Standardizing the features so that they are centered around 0 with a standard deviation of 1 is not only important if we are comparing measurements that have different units, but it is also a general requirement for many machine learning algorithms. \n",
    "\n",
    "<b>Statistics Review</b>: \n",
    "* Remember that the <b>mean</b> is simply the *average* of the values. \n",
    "* The <b>standard deviation</b> is a measure that is used to quantify the amount of variation or dispersion of a set of data values. A low standard deviation indicates that the data points tend to be close to the mean (also called the expected value) of the set, while a high standard deviation indicates that the data points are spread out over a wider range of values. ([Calculating Standard Deviation](https://www.khanacademy.org/math/probability/data-distributions-a1/summarizing-spread-distributions/a/calculating-standard-deviation-step-by-step))\n",
    "* (Source: Understanding [Mean](https://en.wikipedia.org/wiki/Arithmetic_mean) and [Standard Deviation](https://en.wikipedia.org/wiki/Standard_deviation))\n",
    "* [Understanding Normal Distribution.](https://www.mathsisfun.com/data/standard-normal-distribution.html)\n",
    "\n",
    "![alt](images/gaussian_.png)\n",
    "\n",
    "The result of <b>standardization</b> (or <b>Z-score normalization</b>) is that the features will be rescaled so that theyâ€™ll have the properties of a standard normal distribution with zero mean and unit standard deviation. <b>Standard scores</b> (also called <b>z scores</b>) of the samples are calculated as follows:\n",
    "![alt](images/z_score_.png)\n",
    "\n",
    "One way to do this is to use `StandardScaler` as follows:\n",
    "```python\n",
    "standard_scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_standard = standard_scaler.transform(X_train)\n",
    "X_test_standard = standard_scaler.transform(X_test)\n",
    "```\n",
    "If you get an error, simply cast `X_train_standard` back into a DataFrame.\n",
    "```python\n",
    "X_train_standard = pd.DataFrame(X_train_standard, columns = X.columns)\n",
    "```\n",
    "This is because `StandardScaler` returns a numpy array. We want to preserve `X`'s `DataFrame` structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>5.)</b> Scale `X_train` using standardization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_standard = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>6.)</b> To see how our data has transformed:\n",
    "* Inspect `X_train_standard` using `describe()`\n",
    "* Print the first 10 instances of `X_train_standard`\n",
    "\n",
    "Each feature vector should have a mean = 0, variance = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Features to a range\n",
    "For some classifiers, it's useful to scale the features down to a value within the range 0 to 1.\n",
    "\n",
    "A Min-Max scaling is typically done via the following equation:\n",
    "![alt](images/min_max_.png)\n",
    "\n",
    "In scikit-learn, we use `MinMaxScaler` to scale the matrix to a `[0,1]` range:\n",
    "```python \n",
    "minmax_scaler = preprocessing.MinMaxScaler().fit(X_train)\n",
    "X_train_minmax = minmax_scaler.transform(X_train)\n",
    "X_test_minmax = minmax_scaler.transform(X_test)\n",
    "```\n",
    "\n",
    "Again, if you get an error, simply cast `X_train_minmax` back into a DataFrame.\n",
    "```python\n",
    "X_train_minmax = pd.DataFrame(X_train_minmax, columns = X.columns)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale `X_train` so that it is within the range 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_minmax = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>7.)</b> To see how our data has transformed:\n",
    "* Inspect `X_train_minmax` using `describe()`\n",
    "* Print the first 10 instances of `X_train_minmax`\n",
    "\n",
    "The lowest value (min) in each feature vector should be 0. The highest value (max) should be 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now plot the data to see (visually) how the data has been transformed. Here, we use only the first two attributes: Alcohol and Malic acid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot():\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    feature1 = headers[1]\n",
    "    feature2 = headers[2]\n",
    "\n",
    "    plt.scatter(X[feature1], X[feature2], color='green', label='raw input scale', alpha=0.5)\n",
    "    plt.scatter(X_train_standard[feature1], X_train_standard[feature2], color='red', label='Standardized (mean centered on 0, variance = 1)', alpha=0.5)\n",
    "    plt.scatter(X_train_minmax[feature1], X_train_minmax[feature2], color='blue', label='Scaled between 0 to 1 (MinMaxScaler)', alpha=0.5)\n",
    "    plt.xlabel(feature1)\n",
    "    plt.ylabel(feature2)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "# Uncomment this to see the plot\n",
    "#plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright! It's time to compare the performances of the classifiers for the scaled and unscaled data. \n",
    "\n",
    "Note that to compare the performances after applying the different feature scaling techniques, we use cross validation rather than evaluating the test set. This is because if we choose a feature scaling technique by examining the test set, then we may end up choosing a technique that works specifically for that test set; thus the accuracy score derived from the test set will no longer be a good estimate of how well it generalizes to new examples. \n",
    "* k-cross validation cheat sheet: See [(Slide Set 2) K-Nearest Neighbor.pdf](https://github.com/wwcodemanila/WWCodeManila-ML.AI/tree/master/slides)\n",
    "* [More on Cross Validation here.](https://en.wikipedia.org/wiki/Cross-validation_(statistics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def validate(string, X_train, y_train):\n",
    "    print('\\n' + string)\n",
    "\n",
    "    # As we've done before, let's apply our machine learning algorithms!\n",
    "    # 9. Train X using various classifiers of your own choosing.\n",
    "    #    I suggest trying K-Nearest Neighbor (KNN), DecisionTreeClassifier... try others!\n",
    "    #    For each classifier used, calculate and print the cross validation score\n",
    "    #    (using scikit-learn's cross_val_score) along with the classifier's name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Not scaled\n",
      "\n",
      "Standardized\n",
      "\n",
      "Scaled to range 0-1\n"
     ]
    }
   ],
   "source": [
    "validate('Not scaled', X_train, y_train)\n",
    "validate('Standardized', X_train_standard, y_train)\n",
    "validate('Scaled to range 0-1', X_train_minmax, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the <b>classifier</b> and <b>feature scaling technique</b> that yields the best cross validation score, predict `X_test`. \n",
    "* Make sure you scale `X_test` and `X_train` first using the best feature scaling technique from the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "X_test_scaled = None\n",
    "clf = None # Best classifier\n",
    "accuracy = None \n",
    "\n",
    "print accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Share your results! :D\n",
    "\n",
    "This tutorial is based on http://sebastianraschka.com/Articles/2014_about_feature_scaling.html. \n",
    "* This also shows you how to implement scaling manually (equations are given).\n",
    "* If you want a more detailed explanation, check it out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FAQ's\n",
    "\n",
    "1.) Is it always a good idea to scale/normalize our data? Which technique should you use to rescale your data?\n",
    "Answer: \n",
    "- [This answer here is pretty extensive.](http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-16.html)\n",
    "- [Sebastian Raschka's answer](http://sebastianraschka.com/Articles/2014_about_feature_scaling.html#z-score-standardization-or-min-max-scaling)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
