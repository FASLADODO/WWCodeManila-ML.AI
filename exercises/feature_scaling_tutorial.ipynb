{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature scaling\n",
    "\n",
    "Different features can be measured on different scales. \n",
    "\n",
    "> For example:\n",
    "> * height can be measure in centimeters\n",
    "> * weight in kilograms\n",
    "> * blood pressure in mmHg\n",
    "> * etc. \n",
    "\n",
    "Some classifiers combine and compare feature values, e.g. computing distance using Euclidean distance. The problem is that if one of the features has a broad range of values, the distance will be governed by this particular feature!\n",
    "\n",
    "> For example, consider the two features:\n",
    "> * the percentage of unemployment in a city - ranges from 0.0 to 1.0\n",
    "> * the population of the city - can range up to 500,000\n",
    ">\n",
    "> Population will govern percentage by far. \n",
    "\n",
    "Scaling transforms the data so that the features have, more or less, uniform range.\n",
    "\n",
    "> <i>The main advantage of scaling is to avoid attributes in greater numeric ranges dominating those in smaller numeric ranges.</i>\n",
    "\n",
    "As we'll see later, scaling features can lead to a faster optimization process and better results. However, for other algorithms like the Decision Tree Classifier, scaling is *not* necessary (i.e. it's scale-invariant). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wine Dataset\n",
    "About the dataset: `wine.csv`\n",
    "\n",
    "These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines\n",
    "\n",
    "The data consists of:\n",
    "- 13 features (see dataset/wine.headers)\n",
    "- 3 classes - types of wine (I'm not sure what these types particularly are. In the dataset, they're just denoted as 1, 2, and 3. Sarreh.) \n",
    "\n",
    "See more detail here: https://archive.ics.uci.edu/ml/datasets/wine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get started\n",
    "Scikit-learn comes with a package for data preprocessing `sklearn.preprocessing`. Let's start by importing that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>1.)</b> Import as well all libraries that we've used from the previous sessions. We'll also be plotting some things so rememeber to import `matplotlib as plt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2.)</b> Load the dataset wine.csv located in the /datasets folder. Set header=None since the dataset has no headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pandas' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-183-6ef2315bcc32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pandas' is not defined"
     ]
    }
   ],
   "source": [
    "filename = None\n",
    "df = pandas.read_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The headers are actually stored in the file /datasets/wine.names. We load this file as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "headers = []\n",
    "with open('../datasets/wine.names','r') as file:\n",
    "    for line in file:\n",
    "        headers.append(None)\n",
    "        # Append each line to the 'headers' list.\n",
    "        # Important: Strip each line of line breaks(\\n) and trailing white spaces. \n",
    "        # In short, append line.strip() to headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.columns = headers #Sets the headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>3.)</b> Print the first 10 instances of the dataframe using `head()` just to check that we loaded the data and headers properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>4.)</b> Create matrix `X` containing the features and target vector `y` containing the classes.\n",
    "\n",
    "Note that with DataFrames, you can access certain features using their column names/headers. This will be useful for us later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = None\n",
    "y = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization \n",
    "Some machine learning algorithms (e.g. SVM) behave badly when their individual features don't look like standard normally distributed data (i.e. Gaussian distribution with zero mean and unit variance). \n",
    "<center><img src=\"https://docs-assets.developer.apple.com/published/568138fb95/gaussian_labeled_2x_b95dbf3b-42f2-44b8-8034-88835c10b3e4.png\" alt=\"Drawing\" width=\"400\" /> </center>\n",
    "Data scaled using standardization has zero mean and unit variance:\n",
    "\n",
    "The function `scale` provides a quick and easy way to perform this operation on a single array-like dataset:\n",
    "```python \n",
    "X_standard = preprocessing.scale(X)\n",
    "```\n",
    "Another way to do this is to use `StandardScaler` as follows:\n",
    "```python\n",
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "X_standard = scaler.transform(X)\n",
    "```\n",
    "If you get an error, simple cast `X_standard` back into a DataFrame.\n",
    "```python\n",
    "X_standard = pd.DataFrame(X_standard, columns = X.columns)\n",
    "```\n",
    "This is because `StandardScaler` returns a numpy array. We want to preserve X's DataFrame data type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>5.)</b> Scale X using standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_standard = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Standardization')\n",
    "for feature in X_standard:\n",
    "    mean = X_standard[feature].mean() # get mean\n",
    "    variance = X_standard[feature].std() # get variance\n",
    "    print(feature + 'mean = {:.2f}, std = {:.2f}'.format(mean, variance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each feature vector should have a mean = 0, variance = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>6.)</b> Print the first 10 instances of `X_standard` (using `head()`, again) to see how our data has transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Features to a range\n",
    "For some classifiers, it's useful to scale the features down to a value within the range 0 to 1. \n",
    "\n",
    "In scikit-learn, we use `MinMaxScaler` to scale the matrix to a `[0,1]` range:\n",
    "```python \n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "X_minmax = scaler.fit_transform(X)\n",
    "```\n",
    "\n",
    "Again, if you get an error, simply cast `X_minmax` back into a DataFrame.\n",
    "```python\n",
    "X_minmax = pd.DataFrame(X_minmax, columns = X.columns)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale X so that it is within the range 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_minmax = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('\\n\\nMin-max Scaling')\n",
    "for feature in X_minmax:\n",
    "    min_ = X_minmax[feature].min()\n",
    "    max_ = X_minmax[feature].max()\n",
    "    print(feature + 'min = {:.2f}, max = {:.2f}'.format(min_, max_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lowest value (min) in each feature vector should be 0.\n",
    "\n",
    "The highest value (max) should be 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>7.)</b> Print the first 10 instances of X_minmax (using head(), again) to see how our data has transformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now plot the data to see (visually) how the data has been transformed. Here, we use only the first two attributes: Alcohol and Malic acid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot():\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    feature1 = 'Alcohol' #alternatively, you can call headers[1] to avoid hard-coding\n",
    "    feature2 = 'Malic acid' #headers[2]\n",
    "\n",
    "    plt.scatter(X[feature1], X[feature2], color='green', label='Raw input scale', alpha=0.5)\n",
    "    plt.scatter(X_standard[feature1], X_standard[feature2], color='red', label='Standardized', alpha=0.5)\n",
    "    plt.scatter(X_minmax[feature1], X_minmax[feature2], color='blue', label='Scaled between 0 to 1', alpha=0.5)\n",
    "    plt.xlabel(feature1)\n",
    "    plt.ylabel(feature2)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n",
    "plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright! It's time to compare the performances of the classifiers for the scaled and unscaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def classify(string, X):\n",
    "    print('\\n' + string)\n",
    "\n",
    "    # As we've done before, let's apply our machine learning algorithms!\n",
    "\n",
    "    # 8. Split the dataset into training and testing data\n",
    "    #    Set the train size to 80%, and hold back 20% for testing \n",
    "    #    For random_state, set the seed to 7 (para uniform results natin lahat :D).\n",
    "\n",
    "    # 9. Train X using different classifiers.\n",
    "    #    I suggest trying K-Nearest Neighbor (KNN), DecisionTreeClassifier... try others!\n",
    "    #    For each classifier used, calculate the accuracy, then print the accuracy along with the classifier's name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classify('Not scaled', X)\n",
    "classify('Standardized', X_standard)\n",
    "classify('Scaled to range 0 -1', X_minmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Share your results! :D\n",
    "\n",
    "This tutorial is based on http://sebastianraschka.com/Articles/2014_about_feature_scaling.html. \n",
    "* This also shows you how to implement scaling manually (equations are given).\n",
    "* If you want a more detailed explanation, check it out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FAQ's\n",
    "\n",
    "1. Is it always a good idea to scale/normalize our data? Which technique should you use: standardization or normalization?\n",
    "Answer: [This answer here is pretty extensive and sums it up pretty well.](http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-16.html)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
