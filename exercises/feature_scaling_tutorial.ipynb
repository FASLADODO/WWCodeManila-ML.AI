{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature scaling\n",
    "\n",
    "Different features can be measured on different scales. For example:\n",
    " * height can be measure in centimeters\n",
    " * weight in kilograms\n",
    " * blood pressure in mmHg\n",
    " * etc. \n",
    "\n",
    "Some classifiers combine and compare feature values, e.g. computing distance using Euclidean distance. The problem is that if one of the features has a broad range of values, the distance will be governed by this particular feature! For example, consider the two features:\n",
    "* the percentage of unemployment in a city - ranges from 0.0 to 1.0\n",
    "* the population of the city - can range up to 500,000\n",
    "\n",
    "In this example, the percentage will be swamped by the population. \n",
    "\n",
    "<b>Feature scaling</b> transforms the data so that the features have, more or less, uniform range. <i>The main advantage of scaling is to avoid attributes in greater numeric ranges dominating those in smaller numeric ranges.</i>\n",
    "\n",
    "As we'll see later, scaling features can lead to a faster optimization process and better results. However, for other algorithms like the Decision Tree Classifier, scaling is *not* necessary (i.e. it's scale-invariant). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wine Dataset\n",
    "About the dataset: `wine.csv`\n",
    "\n",
    "These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines\n",
    "\n",
    "The data consists of:\n",
    "- 13 features (see dataset/wine.headers)\n",
    "- 3 classes - types of wine (I'm not sure what these types particularly are. In the dataset, they're just denoted as 1, 2, and 3.) \n",
    "\n",
    "See more detail here: https://archive.ics.uci.edu/ml/datasets/wine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get started\n",
    "Scikit-learn comes with a package for data preprocessing `sklearn.preprocessing`. Let's start by importing that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>1.)</b> Import as well all libraries that we've used from the previous sessions. We'll also be plotting some things so rememeber to import `matplotlib as plt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2.)</b> Load the dataset `wine.csv` located in the /datasets folder. Set `header=None` since the dataset has no headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = None\n",
    "df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The headers are actually stored in the file /datasets/wine.names. We load this file as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "headers = []\n",
    "with open('../datasets/wine.names','r') as file:\n",
    "    for line in file:\n",
    "        headers.append(line.strip()) # appends each line to the 'headers' list. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`strip()` strips each line of line breaks (\\n) and trailing white spaces.\n",
    "\n",
    "We can then set the headers as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-233-23d83d9b83dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mheaders\u001b[0m \u001b[1;31m#Sets the headers\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "df.columns = headers #Sets the headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>3.)</b> Inspect your data using methods found in the following tutorial: [Understand your Data with Descriptive Statistics](http://machinelearningmastery.com/understand-machine-learning-data-descriptive-statistics-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>4.)</b> Create matrix `X` containing the features and target vector `y` containing the classes.\n",
    "\n",
    "Note that with `DataFrames`, you can access certain features using their column names/headers. This will be useful for us later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = None\n",
    "y = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization \n",
    "Some machine learning algorithms behave badly when their individual features don't look like standard normally distributed data (i.e. Gaussian distribution with zero mean and unit standard deviation). Standardizing the features so that they are centered around 0 with a standard deviation of 1 is not only important if we are comparing measurements that have different units, but it is also a general requirement for many machine learning algorithms. \n",
    "\n",
    "<b>Statistics Review</b>: \n",
    "* Remember that the <b>mean</b> is simply the *average* of the values. \n",
    "* The <b>standard deviation</b> is a measure that is used to quantify the amount of variation or dispersion of a set of data values. A low standard deviation indicates that the data points tend to be close to the mean (also called the expected value) of the set, while a high standard deviation indicates that the data points are spread out over a wider range of values. ([Calculating Standard Deviation](https://www.khanacademy.org/math/probability/data-distributions-a1/summarizing-spread-distributions/a/calculating-standard-deviation-step-by-step))\n",
    "* (Source: Understanding [Mean](https://en.wikipedia.org/wiki/Arithmetic_mean) and [Standard Deviation](https://en.wikipedia.org/wiki/Standard_deviation))\n",
    "\n",
    "![alt](images/gaussian_.png)\n",
    "\n",
    "The result of <b>standardization</b> (or <b>Z-score normalization</b>) is that the features will be rescaled so that theyâ€™ll have the properties of a standard normal distribution with zero mean and unit standard deviation. <b>Standard scores</b> (also called <b>z scores</b>) of the samples are calculated as follows:\n",
    "![alt](images/z_score_.png)\n",
    "\n",
    "The function `scale` provides a quick and easy way to perform this operation on a single array-like dataset:\n",
    "```python \n",
    "X_standard = preprocessing.scale(X)\n",
    "```\n",
    "Another way to do this is to use `StandardScaler` as follows:\n",
    "```python\n",
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "X_standard = scaler.transform(X)\n",
    "```\n",
    "If you get an error, simply cast `X_standard` back into a DataFrame.\n",
    "```python\n",
    "X_standard = pd.DataFrame(X_standard, columns = X.columns)\n",
    "```\n",
    "This is because `StandardScaler` returns a numpy array. We want to preserve `X`'s `DataFrame` structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>5.)</b> Scale X using standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_standard = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Standardization')\n",
    "for feature in X_standard:\n",
    "    mean = X_standard[feature].mean() # get mean\n",
    "    variance = X_standard[feature].std() # get variance\n",
    "    print(feature + 'mean = {:.2f}, std = {:.2f}'.format(mean, variance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each feature vector should have a mean = 0, variance = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>6.)</b> You may also inspect your data using `describe()`; then print the first 10 instances of `X_standard` (using `head()`, again) to see how our data has transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Features to a range\n",
    "For some classifiers, it's useful to scale the features down to a value within the range 0 to 1.\n",
    "\n",
    "A Min-Max scaling is typically done via the following equation:\n",
    "![alt](images/min_max_.png)\n",
    "\n",
    "In scikit-learn, we use `MinMaxScaler` to scale the matrix to a `[0,1]` range:\n",
    "```python \n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "X_minmax = scaler.fit_transform(X)\n",
    "```\n",
    "\n",
    "Again, if you get an error, simply cast `X_minmax` back into a DataFrame.\n",
    "```python\n",
    "X_minmax = pd.DataFrame(X_minmax, columns = X.columns)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale X so that it is within the range 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_minmax = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('\\n\\nMin-max Scaling')\n",
    "for feature in X_minmax:\n",
    "    min_ = X_minmax[feature].min()\n",
    "    max_ = X_minmax[feature].max()\n",
    "    print(feature + 'min = {:.2f}, max = {:.2f}'.format(min_, max_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lowest value (min) in each feature vector should be 0. The highest value (max) should be 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>7.)</b> Inpsect your data using `describe()` and print the first 10 instances of `X_minmax` (using `head()`, again) to see how our data has transformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now plot the data to see (visually) how the data has been transformed. Here, we use only the first two attributes: Alcohol and Malic acid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot():\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    feature1 = 'Alcohol' #alternatively, you can call headers[1] to avoid hard-coding\n",
    "    feature2 = 'Malic acid' #headers[2]\n",
    "\n",
    "    plt.scatter(X[feature1], X[feature2], color='green', label='Raw input scale', alpha=0.5)\n",
    "    plt.scatter(X_standard[feature1], X_standard[feature2], color='red', label='Standardized', alpha=0.5)\n",
    "    plt.scatter(X_minmax[feature1], X_minmax[feature2], color='blue', label='Scaled between 0 to 1', alpha=0.5)\n",
    "    plt.xlabel(feature1)\n",
    "    plt.ylabel(feature2)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n",
    "plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright! It's time to compare the performances of the classifiers for the scaled and unscaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def classify(string, X):\n",
    "    print('\\n' + string)\n",
    "\n",
    "    # As we've done before, let's apply our machine learning algorithms!\n",
    "\n",
    "    # 8. Split the dataset into training and testing data\n",
    "    #    Set the train size to 80%, and hold back 20% for testing \n",
    "    #    For random_state, set the seed to 7 (para uniform results natin lahat :D).\n",
    "\n",
    "    # 9. Train X using different classifiers.\n",
    "    #    I suggest trying K-Nearest Neighbor (KNN), DecisionTreeClassifier... try others!\n",
    "    #    For each classifier used, calculate and print the accuracy along with the classifier's name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classify('Not scaled', X)\n",
    "classify('Standardized', X_standard)\n",
    "classify('Scaled to range 0 -1', X_minmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Share your results! :D\n",
    "\n",
    "This tutorial is based on http://sebastianraschka.com/Articles/2014_about_feature_scaling.html. \n",
    "* This also shows you how to implement scaling manually (equations are given).\n",
    "* If you want a more detailed explanation, check it out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FAQ's\n",
    "\n",
    "1.) Is it always a good idea to scale/normalize our data? Which technique should you use to rescale your data?\n",
    "\n",
    "Answer: \n",
    "- [This answer here is pretty extensive.](http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-16.html)\n",
    "- [The answer here is also pretty insightful](http://sebastianraschka.com/Articles/2014_about_feature_scaling.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
