{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we'll tackle the classic iris plant classification problem using Scikit-learn. Think of this as your 'Hello World!' to machine learning. :D\n",
    "\n",
    "The Iris dataset was used in Fisher's classic 1936 paper, The Use of Multiple Measurements in Taxonomic Problems. The data set consists of 50 samples from each of three species of Iris. Each iris plant listed in the dataset has four different features or attributes:\n",
    "\n",
    "- Sepal Length\n",
    "- Sepal Width\n",
    "- Petal Length\n",
    "- Petal Width\n",
    "\n",
    "Our task is to classify the iris plants into 3 species:\n",
    "- Iris Setosa\n",
    "- Iris Versicolour\n",
    "- Iris Virginica \n",
    "\n",
    "![alt](images/iris_.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing libraries\n",
    "We'll start by importing some import libraries for data analysis and scientific computing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Pandas](http://pandas.pydata.org/) provides easy-to-use data structures and data analysis tools for the Python programming language and allows for high-level data manipulation. Meanwhile [NumPy](http://www.numpy.org/), which stands for Numerical Python, is a scientific computing package.\n",
    "\n",
    "Next, we'll import important tools and models from the Scikit-learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these four lines, we import the four machine learning algorithms we'll be using in this example. In future sessions, we'll get to learn how some of these algorithms works, but for now, you can view these models as a 'black box' which accepts some input (features/attributes) and produces some output (predictions), without any knowledge of its internal workings. \n",
    "\n",
    "<b>FAQ</b>: How do you know which algorithm to choose and which will work well for your problem?\n",
    "\n",
    "<b>Answer</b>: Generally, there are a lot of algorithms to choose from. [See this guide for a tour of the different ML algorithms](https://www.quora.com/How-do-you-choose-a-machine-learning-algorithm). \n",
    "\n",
    "To choose an appropriate algorithm, you first need to understand and categorize the problem you are trying to solve. [See this post for a detailed explanation](https://www.quora.com/How-do-you-choose-a-machine-learning-algorithm). \n",
    "For a short version, what you need to do to choose the right algorithms are:\n",
    "1. Categorize your problem (supervised, unsupervised, reinforcement, classification, regression, sequential/temporal, etc.)\n",
    "2. Find available algorithms applicable to your problem (e.g. SVM, ANN work for classification problems; CRF, HMM for sequential data, etc.)\n",
    "3. Implement them by setting up a machine learning pipeline that compares the performances of the different algorithms\n",
    "4. Optimize hyperparameters (optional)\n",
    "\n",
    "In fact, you can can look at this [scikit-learn cheat sheet](https://i.stack.imgur.com/BZJiN.png) for a very rough guide to choosing an algorithm for your problem (just don't limit the models you use to the algorithms in the cheat sheet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two lines are tools for automatically splitting the dataset and computing the accuracy of the resulting predictions, respectively. We'll get to see these two tools in action later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading and Inspecting the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = '../datasets/iris.csv' \n",
    "dataframe = pd.read_csv(filename, header=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright. So here, we've loaded the dataset. This piece of code locates the `iris.csv` file in the `datasets` folder. We set `header = 0` to specify that the the first row (row 0 in Python) contains the headers, i.e. the names of each column (e.g. Sepal Length, Sepal Width, ... , Species). Some datasets might not contain headers, so be careful with this!\n",
    "\n",
    "Afterwards, we store the dataset in the variable `dataframe` as a Pandas data structure called the [DataFrame](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html). You can think of a DataFrame as a two-dimensional array or matrix containing our data. [You can also look at this cheat sheet for a visualization]().\n",
    "\n",
    "Now, let's inspect our data. It's always a good idea to know what your data looks like. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 5)\n"
     ]
    }
   ],
   "source": [
    "print(dataframe.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we check the dimensions of the data. There are:\n",
    "- 150 rows (50 samples for each of the the 3 species) \n",
    "- 5 columns (Sepal Length, Sepal Width, Petal Length, Petal Width, and Species)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peeking at your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`head(10)` prints the first 10 rows or instances of your data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n",
      "0            5.1           3.5            1.4           0.2  Iris-setosa\n",
      "1            4.9           3.0            1.4           0.2  Iris-setosa\n",
      "2            4.7           3.2            1.3           0.2  Iris-setosa\n",
      "3            4.6           3.1            1.5           0.2  Iris-setosa\n",
      "4            5.0           3.6            1.4           0.2  Iris-setosa\n",
      "5            5.4           3.9            1.7           0.4  Iris-setosa\n",
      "6            4.6           3.4            1.4           0.3  Iris-setosa\n",
      "7            5.0           3.4            1.5           0.2  Iris-setosa\n",
      "8            4.4           2.9            1.4           0.2  Iris-setosa\n",
      "9            4.9           3.1            1.5           0.1  Iris-setosa\n"
     ]
    }
   ],
   "source": [
    "print(dataframe.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tail(10)` prints the last 10 rows or instances of your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm         Species\n",
      "140            6.7           3.1            5.6           2.4  Iris-virginica\n",
      "141            6.9           3.1            5.1           2.3  Iris-virginica\n",
      "142            5.8           2.7            5.1           1.9  Iris-virginica\n",
      "143            6.8           3.2            5.9           2.3  Iris-virginica\n",
      "144            6.7           3.3            5.7           2.5  Iris-virginica\n",
      "145            6.7           3.0            5.2           2.3  Iris-virginica\n",
      "146            6.3           2.5            5.0           1.9  Iris-virginica\n",
      "147            6.5           3.0            5.2           2.0  Iris-virginica\n",
      "148            6.2           3.4            5.4           2.3  Iris-virginica\n",
      "149            5.9           3.0            5.1           1.8  Iris-virginica\n"
     ]
    }
   ],
   "source": [
    "print(dataframe.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Data Type for each Attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SepalLengthCm    float64\n",
      "SepalWidthCm     float64\n",
      "PetalLengthCm    float64\n",
      "PetalWidthCm     float64\n",
      "Species           object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(dataframe.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get an idea of the types of attributes by peeking at the raw data, as above. You can also list the data types used by the DataFrame to characterize each attribute using the dtypes property.\n",
    "- `float64` refers to numerical data (particularly decimal numbers) \n",
    "- `object` in this dataset pertains to data with string values\n",
    "\n",
    "[Check out the other types of data here.](https://docs.scipy.org/doc/numpy-1.12.0/user/basics.types.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive Statistics\n",
    "\n",
    "Descriptive statistics can give you great insight into the shape of each attribute.\n",
    "\n",
    "Often you can create more summaries than you have time to review. The describe() function on the Pandas DataFrame lists 8 statistical properties of each attribute:\n",
    "- Count\n",
    "- Mean\n",
    "- Standard Deviation\n",
    "- Minimum Value\n",
    "- 25th Percentile\n",
    "- 50th Percentile (Median)\n",
    "- 75th Percentile\n",
    "- Maximum Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm\n",
      "count     150.000000    150.000000     150.000000    150.000000\n",
      "mean        5.843333      3.054000       3.758667      1.198667\n",
      "std         0.828066      0.433594       1.764420      0.763161\n",
      "min         4.300000      2.000000       1.000000      0.100000\n",
      "25%         5.100000      2.800000       1.600000      0.300000\n",
      "50%         5.800000      3.000000       4.350000      1.300000\n",
      "75%         6.400000      3.300000       5.100000      1.800000\n",
      "max         7.900000      4.400000       6.900000      2.500000\n"
     ]
    }
   ],
   "source": [
    "print(dataframe.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Distribution (Classes only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On classification problems you need to know how balanced the class values are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Species\n",
      "Iris-setosa        50\n",
      "Iris-versicolor    50\n",
      "Iris-virginica     50\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(dataframe.groupby('Species').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A balanced dataset is where the number of observations/instances for each class are more or less the same. The Iris dataset is an example of a balanced dataset since each class has exactly 50 observations.\n",
    "\n",
    "Highly imbalanced problems are problems wherein there are a lot more observations for one class than another. These are common and may need special handling in the data preparation stage of your project. We'll discuss ways to handle this in future sessions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation between Attributes\n",
    "Correlation refers to the relationship between two variables and how they may or may not change together.\n",
    "\n",
    "The most common method for calculating correlation is Pearson’s Correlation Coefficient, that assumes a normal distribution of the attributes involved. A correlation of -1 or 1 shows a full negative or positive correlation respectively. Whereas a value of 0 shows no correlation at all.\n",
    "\n",
    "Some machine learning algorithms like linear and logistic regression can suffer poor performance if there are highly correlated attributes in your dataset. As such, it is a good idea to review all of the pair-wise correlations of the attributes in your dataset. You can use the corr() function on the Pandas DataFrame to calculate a correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm\n",
      "SepalLengthCm       1.000000     -0.109369       0.871754      0.817954\n",
      "SepalWidthCm       -0.109369      1.000000      -0.420516     -0.356544\n",
      "PetalLengthCm       0.871754     -0.420516       1.000000      0.962757\n",
      "PetalWidthCm        0.817954     -0.356544       0.962757      1.000000\n"
     ]
    }
   ],
   "source": [
    "print(dataframe.corr(method='pearson'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix lists all attributes across the top and down the side, to give correlation between all pairs of attributes (twice, because the matrix is symmetrical). You can see the diagonal line through the matrix from the top left to bottom right corners of the matrix shows perfect correlation of each attribute with itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extracting Features and Labels\n",
    "Next, we need to exract the features/attributes as well as our class labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = dataframe.iloc[0:150, 0:4] \n",
    "y = dataframe.iloc[0:150, 4] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that we have four features (Sepal Length, Sepal Width, Petal Length, and Petal Width) in the first four columns and the class label ('Species') in the fifth (last) column.\n",
    "\n",
    "<b>FAQ</b>: What's `iloc`?\n",
    "\n",
    "<b>Answer</b> We use `iloc` (which stands for '<b>i</b>nteger <b>loc</b>ation') to select certain parts of the data based on their position or index. \n",
    "\n",
    "<b>Recap on Python indexing</b>: Remember that in Python, indexing starts at zero (0) and ends at n-1 where n is the maximum number of elements. Thus, for the Iris dataset containing 150 rows, the first row is considered to be at index 0 and the last row is at index 149. Similarly, for the columns, we have 5 columns with indices 0, 1, 2, 3, and 4. \n",
    "\n",
    "<b>Recap on Python Slicing</b>: Remember that in Python, when slicing a list or matrix, we need to specify the starting index and the ending index in square brackets separated by a colon( e.g. `[starting_index : ending_index]`). Python however returns the list of elements from the starting index up until the ending index - 1. (Yeah, Python's a little eff'd up like.)\n",
    "\n",
    "Also, if you don't specify the starting index, it automatically starts at the very beginning which is at index `0` (e.g. `[:5]` is the same as `[0:5]`). Likewise, if you don't specify the ending index, it gets everything from the starting index up to the very end (e.g. if you have 4 elements, then `[1:]` is the same as `[1:5]`). Meaning, if you have something like `[:]`, this basically gets ALL the elements. Kapische? \n",
    "\n",
    "[For more information on basic Python indexing and slicing for lists, see this very helpful tutorial!](https://www.tutorialspoint.com/python/python_lists.htm)\n",
    "\n",
    "<b> Slicing for DataFrames</b>\n",
    "\n",
    "In the first line, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = dataframe.iloc[0:150, 0:4] #or dataframe.iloc[:, :4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we create a variable `X` containing a matrix of the features of all the rows. That is, we slice the dataframe using two sets of colons: the first set is for the <b>rows</b>, the second for the <b>columns</b>. In other words, `0:150` indicates that we want to get all the rows from index 0 until index 149. That's basically all the rows in our dataset! Then, `0:4` indicates we want to get all the columns from index 0 up to index 3 (excluding index 4). Thus, `dataframe.iloc[0:150, 0:4]` is the same as `dataframe.iloc[:, :4]`.\n",
    "\n",
    "![](images/slice1.png)\n",
    "\n",
    "In the second line, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = dataframe.iloc[0:150, 4] #or dataframe.iloc[:, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we create a variable `y` containing an array of the class labels of all the rows. Since the labels are located at index 4, we slice the DataFrame such that we get all the rows at the 4th index. \n",
    "![](images/slice2.png)\n",
    "\n",
    "(<b>Note</b>: Error in the figure - 151 should be 149.)\n",
    "\n",
    "Lastly, note that we can also access the columns values using the header names or labels using `loc` instead of `iloc`.\n",
    "For more info, [try reading up on it here to get a better understanding](https://pandas.pydata.org/pandas-docs/stable/indexing.html#selection-by-position)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Splitting the dataset\n",
    "Some models may perform better than others on certain datasets. By 'perform', we are pertaining to how well a model can predict new data that it has not yet seen before. How then do we quantify the performance of a model?\n",
    "\n",
    "A general practice is to split your data into a training and test set. \n",
    "- The training set is used train/tune your model in order for it to learn your data.\n",
    "- The test set is used to evaluate how well it generalizes to data it has never seen before. We can use a certain metric (e.g. accuracy, error measures) to quanitify the performance of the model on the test set.\n",
    "(Source: [Why split into training and test set?](https://www.quora.com/What-is-a-training-data-set-test-data-set-in-machine-learning-What-are-the-rules-for-selecting-them))\n",
    "\n",
    "In essence, rather than using the entire dataset to train your model, you <i>hold back</i> a part of your dataset with the goal of eventually coming up with some measurement (e.g. accuracy, error) of how well your model performs on never-before-seen-data (the test set). For example, you wouldn't want to use a model with only a 51% accuracy. That's hardly better than random guessing!\n",
    "\n",
    "Note that in splitting the dataset, the training set must be much <i>much</i> larger than the training set. This is because machine learning models generally depend on a lot of data to be able to generalize well. By holding back data for testing, we're essentially reducing the amount valuable information for the model to learn. Therefore, we usually allot more data to the training set; a typical split is 60/40, 70/30, 80/20 with the larger portion alotted for the training set.\n",
    "\n",
    "Alright. So let's start splitting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_size = 0.8\n",
    "seed = 7 \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, there's a method called `train_test_split` that automatically splits the dataset. Here, we set the `train_size` to 80%, which automatically allots the remaining 20% to the test set. \n",
    "\n",
    "<b>FAQ:</b> What's the <i>seed</i> or <i>random state</i>?\n",
    "\n",
    "<b>Answer</b>: [This answer sums it up pretty nicely :)](https://stackoverflow.com/questions/42191717/python-random-state-in-splitting-dataset)\n",
    "Short answer: It's used for reproducibility and debugging. It can be set to any integer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Instantiate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instantiate learning model\n",
    "clf = GaussianNB()\n",
    "\n",
    "# Fit model to training set\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels of the test set\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the Gaussian Naive Bayes Model denoted as `GaussianNB()`. We then fit the classifier model to the training set using the `fit()` method. Finally, we use `predict()` to produce predicted labels for the test set. \n",
    "\n",
    "We can compare the predicted labels `y_pred` and true test labels `y_test` manually as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Predicted label       True label\n",
      "149   Iris-virginica   Iris-virginica\n",
      "84   Iris-versicolor  Iris-versicolor\n",
      "40       Iris-setosa      Iris-setosa\n",
      "66   Iris-versicolor  Iris-versicolor\n",
      "106  Iris-versicolor   Iris-virginica\n",
      "41       Iris-setosa      Iris-setosa\n",
      "52    Iris-virginica  Iris-versicolor\n",
      "94   Iris-versicolor  Iris-versicolor\n",
      "11       Iris-setosa      Iris-setosa\n",
      "51   Iris-versicolor  Iris-versicolor\n",
      "77    Iris-virginica  Iris-versicolor\n",
      "85   Iris-versicolor  Iris-versicolor\n",
      "32       Iris-setosa      Iris-setosa\n",
      "109   Iris-virginica   Iris-virginica\n",
      "28       Iris-setosa      Iris-setosa\n",
      "70    Iris-virginica  Iris-versicolor\n",
      "108   Iris-virginica   Iris-virginica\n",
      "137   Iris-virginica   Iris-virginica\n",
      "46       Iris-setosa      Iris-setosa\n",
      "37       Iris-setosa      Iris-setosa\n",
      "82   Iris-versicolor  Iris-versicolor\n",
      "120   Iris-virginica   Iris-virginica\n",
      "63   Iris-versicolor  Iris-versicolor\n",
      "119  Iris-versicolor   Iris-virginica\n",
      "129   Iris-virginica   Iris-virginica\n",
      "138   Iris-virginica   Iris-virginica\n",
      "97   Iris-versicolor  Iris-versicolor\n",
      "80   Iris-versicolor  Iris-versicolor\n",
      "101   Iris-virginica   Iris-virginica\n",
      "140   Iris-virginica   Iris-virginica\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame({'Predicted label': y_pred, 'True label': y_test})\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check that our model has not predicted the test set perfectly and commits some mistakes. \n",
    "\n",
    "We want to get the percentage of the `test_set` wherein the predicted label is the same as the true label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Naive Bayes: 0.833333333333\n"
     ]
    }
   ],
   "source": [
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred) # or accuracy = np.mean(y_pred == y_test) \n",
    "print(\"Gaussian Naive Bayes: \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Naive Bayes: 0.833333333333\n"
     ]
    }
   ],
   "source": [
    "# Or, more concisely (predicts y_pred and evaluates accuracy in a single line of code)\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "print(\"Gaussian Naive Bayes: \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And voila! Using the Gaussian Naive Bayes classifier we get a whooping 83% accuracy. Can we do better though?\n",
    "\n",
    "Let's try comparing the other different classifiers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classifier: 0.9\n",
      "Support Vector Machines: 0.933333333333\n",
      "Gaussian Naive Bayes: 0.833333333333\n"
     ]
    }
   ],
   "source": [
    "# To evaluate multiple classifiers, store classifiers into a dictionary:\n",
    "classifiers = dict() \n",
    "classifiers['Gaussian Naive Bayes'] = GaussianNB()\n",
    "classifiers['Decision Tree Classifier'] = DecisionTreeClassifier(random_state=seed)\n",
    "classifiers['Support Vector Machines'] = SVC()\n",
    "\n",
    "# Iterate over dictionary\n",
    "for clf_name, clf in classifiers.items(): #clf_name is the key, clf is the value\n",
    "    clf.fit(X_train, y_train)\n",
    "    accuracy = clf.score(X_test, y_test)\n",
    "    print(clf_name + ': ' + str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we see that Support Vector Machines (SVM) produces the best results with a 93% accuracy!\n",
    "\n",
    "In the next session, we'll learn about and implement our first machine learning algorithm, K-Near Neighbor (KNN). "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
